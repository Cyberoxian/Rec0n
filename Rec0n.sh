#!/bin/bash

    printf "\n"
    printf "\e[1m\e[31m%s\e[0m\n" "            _"
    printf "\e[1m\e[32m%s\e[0m\n" "            / \\      _-'"
    printf "\e[1m\e[33m%s\e[0m\n" "          _/|  \\-''- _ /"
    printf "\e[1m\e[34m%s\e[0m\n" "       __-' { |          \\ "
    printf "\e[1m\e[35m%s\e[0m\n" "          /             \\"
    printf "\e[1m\e[36m%s\e[0m\n" "         /       \"o.  |o }"
    printf "\e[1m\e[37m%s\e[0m\n" "         |            \\ ;"
    printf "\e[1m\e[91m%s\e[0m\n" "                       ',    </ScR1Pt> Advanced WebApp Recon Tool </ScR1Pt>"
    printf "\e[1m\e[92m%s\e[0m\n" "            \\_         __\\"
    printf "\e[1m\e[93m%s\e[0m\n" "              ''-_    \\.//"
    printf "\e[1m\e[94m%s\e[0m\n" "                / '-____'"
    printf "\e[1m\e[95m%s\e[0m\n" "               /"
    printf "\e[1m\e[96m%s\e[0m\n" "             _'"
    printf "\e[1m\e[97m%s\e[0m\n" "           _-'" 
    printf "\n"
    printf "\n\e[1m\e[32m%s\e[0m\n" "                             [*] Author ~Rajiv Sharma-0x14 (v.2)"
    printf "\n"
    printf "\n"
	
start_time=$(date +%s)

url=$1
if [ ! -d "$url" ];then
	mkdir $url
fi
if [ ! -d "$url/recon" ];then
	mkdir $url/recon
fi
if [ ! -d "$url/recon/scans" ];then
	mkdir $url/recon/scans
fi
if [ ! -d "$url/recon" ];then
	mkdir $url/recon/screenshot
fi
if [ ! -d "$url/recon/gf" ];then
	mkdir $url/recon/gf
fi
if [ ! -d "$url/recon" ];then
	mkdir $url/recon
fi
if [ ! -f "$url/recon/alive.txt" ];then
	touch $url/recon/alive.txt
fi
if [ ! -f "$url/recon/subdomain.txt" ];then
	touch $url/recon/subdomain.txt
fi
## Crt.sh
echo -e "\e[31m[+]\e[0m \e[33mHarvesting subdomains with crt.sh ‚öôÔ∏è ...\e[0m" #Crt.sh

requestsearch="$(curl -s "https://crt.sh?q=%.$url&output=json")"
echo $requestsearch > req.txt
cat req.txt | jq ".[].common_name,.[].name_value"| cut -d'"' -f2 | sed 's/\\n/\n/g' | sed 's/\*.//g'| sed -r 's/([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,4})//g' | sort | uniq > $url/recon/crt.txt
rm req.txt
echo ""
cat $url/recon/crt.txt
echo ""
cat $url/recon/crt.txt | grep $1 >> $url/recon/subdomain.txt
echo -e "\e[32m[+]\e[0m Crt.sh Discovered \e[31m"$(cat $url/recon/crt.txt | wc -l)"\e[0m subdomains"
rm $url/recon/crt.txt

## Assetfinder
echo -e "\n\e[31m[+]\e[0m \e[33mHarvesting subdomains with assetfinder ‚öôÔ∏è ...\e[0m" #Assetfinder

assetfinder $url >> $url/recon/assets.txt
cat $url/recon/assets.txt | grep $1 >> $url/recon/subdomain.txt
echo -e "\n\e[32m[+]\e[0m Assetfinder Discovered \e[31m"$(cat $url/recon/assets.txt | wc -l)"\e[0m subdomains"
rm $url/recon/assets.txt

## Subfinder
echo -e "\n\e[31m[+]\e[0m \e[33mDouble checking for subdomains with Subfinder üî• ...\e[0m"

subfinder -d $url | tee -a $url/recon/f.txt
cat $url/recon/f.txt | grep $1 >> $url/recon/subdomain.txt
echo -e "\e[32m[+]\e[0m Subfinder Discovered \e[31m"$(cat $url/recon/f.txt | wc -l)"\e[0m subdomains"
rm $url/recon/f.txt
echo -e "\e[32m[+]\e[0m Total Number of Subdomain's Discovered \e[31m"$(cat $url/recon/subdomain.txt | wc -l)"\e[0m from $url"

## HTTProbe
echo -e "\n\e[31m[+]\e[0m \e[33m Probing for alive domains üïµÔ∏è ...\e[0m" ## Install Manually by using pimpmykali tool(For Kali Linux)

cat $url/recon/subdomain.txt | sort | uniq | httprobe -s -p https:443 | sed 's/https\?:\/\///' | tr -d ':443' >> $url/recon/a.txt
sort -u $url/recon/a.txt > $url/recon/alive.txt

echo -e "\e[32m[+]\e[0m Total Number of Alive domain's Discovered \e[31m"$(cat $url/recon/alive.txt | wc -l)"\e[0m from $url"
rm $url/recon/a.txt

## Host IPv4/IPv6
echo -e "\n\e[31m[+]\e[0m \e[33m Host with IPv4/IPv6 üßê ...\e[0m"
cat "$url/recon/alive.txt" | xargs -I{} host {} | tee -a "$url/recon/host.txt"
echo -e "\n\e[32m[+]\e[0m Host Discovered \e[31m"$(cat $url/recon/host.txt | wc -l)"\e[0m IPv4/IPv6"

## HTTPX-Toolkit

echo -e "\n\e[31m[+]\e[0m \e[33m Checking Status Code of Alive Domains üî• ...\e[0m"
cat $url/recon/alive.txt | httpx-toolkit -sc >> $url/recon/status.txt
cat $url/recon/status.txt |grep "200" >> $url/recon/200.txt
cat $url/recon/status.txt |grep "403" >> $url/recon/403.txt
cat $url/recon/status.txt |grep "404" >> $url/recon/404.txt
#tr used to remove content
#sed used to remove last 3 characters
#If error occur replace [[32m200[0m] into 200 and remove sed
#If error occur replace [[31m403[0m] into 403 and remove sed
cat $url/recon/200.txt | tr -d '[[32200[0]'| sed 's/.\{3\}$//' >> $url/recon/200_live.txt 
cat $url/recon/403.txt | tr -d '[[31403[0]'| sed 's/.\{3\}$//' >> $url/recon/403_live.txt 
rm $url/recon/200.txt
rm $url/recon/403.txt
rm $url/recon/404.txt
rm $url/recon/status.txt

## Aquatone
echo -e "\n\e[31m[+]\e[0m \e[33m Scanning for Aquatone üîç ...\e[0m"
curl -s https://crt.sh/\?q\=%25.$url\&output\=json | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u | httprobe -c  100 | aquatone -out $url/recon/screenshot

#HTTPX-Toolkit

echo -e "\n\e[31m[+]\e[0m \e[33m Checking Status Code on Terminal üîç ...\e[0m"
cat $url/recon/alive.txt | httpx-toolkit -title -wc -sc -cl -ct -cname -web-server -threads 75 -location


## Scanning Port
echo -e "\n\e[31m[+]\e[0m \e[33m Scanning for open ports üîç ...\e[0m"
nmap -A -iL $url/recon/alive.txt -T4 -oA $url/recon/scans/scanned.txt 

#URL's
echo -e "\n\e[31m[+]\e[0m \e[33m If Any Error Occur Leave it and stay calm...\e[0m"
echo -e "\n\e[31m[+]\e[0m \e[33m This May Take A While. Take Coffee and chill...\e[0m"

echo -e "\n\e[31m[+]\e[0m \e[33m Running WaybackUrls üî• ...\e[0m"
cat $url/recon/200_live.txt | waybackurls >> $url/recon/Old_Link.txt

echo -e "\n\e[31m[+]\e[0m \e[33m Running Gather All URL's(GAU) üî• ...\e[0m"
cat $url/recon/200_live.txt | gau >> $url/recon/Old_Link.txt
cat $url/recon/Old_Link.txt | sort -u >> $url/recon/urls.txt
rm  $url/recon/Old_Link.txt

# GF Patterns
#go install github.com/tomnomnom/gf@latest
#git clone https://github.com/1ndianl33t/Gf-Patterns
#sudo cp ~/go/bin/gf /bin/
#mkdir .gf
#mv ~/Gf-Patterns/*.json ~/.gf
echo -e "\n\e[31m[+]\e[0m \e[33m Running GF Patterns üöÄ ...\e[0m"

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For IDOR üî™ ..."
cat $url/recon/urls.txt | gf idor >> $url/recon/gf/IDOR.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For RCE üî™ ..."
cat $url/recon/urls.txt | gf rce >> $url/recon/gf/RCE.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For XSS üî™ ..."
cat $url/recon/urls.txt | gf xss >> $url/recon/gf/XSS.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For SQLI üî™ ..."
cat $url/recon/urls.txt | gf sqli >> $url/recon/gf/SQLI.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For Redirect üî™ ..."
cat $url/recon/urls.txt | gf redirect >> $url/recon/gf/Redirect.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For LFI üî™ ..."
cat $url/recon/urls.txt | gf lfi >> $url/recon/gf/LFI.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For SSRF üî™ ..."
cat $url/recon/urls.txt | gf ssrf >> $url/recon/gf/SSRF.txt

printf "\n\e[1m\e[32m%s\e[0m\n" "Checking For SSTI üî™ ..."
cat $url/recon/urls.txt | gf ssti >> $url/recon/gf/SSTI.txt


end_time=$(date +%s)
execution_time=$((end_time - start_time))

# Display completion message and execution time
printf "\n\e[1m\e[32m%s\e[0m\n" "Recon Completed Successfully! ‚ù§Ô∏è"
printf "\e[1m\e[32m%s\e[0m\n" "Total execution time: $execution_time seconds"

